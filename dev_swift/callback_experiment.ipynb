{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Callback Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "\n",
    "struct DataBatch {\n",
    "    // Simplifying assumption: Model inputs and outputs are Tensor<Float>\n",
    "    var xb: Tensor<Float>\n",
    "    var yb: Tensor<Float>\n",
    "}\n",
    "\n",
    "struct Data {\n",
    "    // Simplifying assumption: Batches are in an array.\n",
    "    var trainBatches: [DataBatch]\n",
    "}\n",
    "\n",
    "enum CallbackResult {\n",
    "    case proceed\n",
    "    case skip\n",
    "    case stop\n",
    "}\n",
    "\n",
    "enum CallbackEvent {\n",
    "    // I haven't implemented all the events.\n",
    "    case beginFit\n",
    "    case beginEpoch\n",
    "    case beginBatch\n",
    "    case afterForwardsBackwards\n",
    "}\n",
    "\n",
    "class Callback<Opt: Optimizer>\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input == Tensor<Float>,\n",
    "      Opt.Model.Output == Tensor<Float> {\n",
    "    func apply(event: CallbackEvent, learner: Learner<Opt>) -> CallbackResult {\n",
    "        return .proceed\n",
    "    }\n",
    "}\n",
    "\n",
    "class Learner<Opt: Optimizer>\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input == Tensor<Float>,\n",
    "      Opt.Model.Output == Tensor<Float>\n",
    "{\n",
    "    typealias Model = Opt.Model\n",
    "    var model: Model\n",
    "    \n",
    "    // I'm getting some crashes in AD-generated code if I put a `lossFunc` in the learner.\n",
    "    // So I'm putting a `lossWithGradient` for now, to work around this.\n",
    "    // (model, context, inputs, labels) -> (loss, grad)\n",
    "    typealias LossWithGradient = (Model, Context, Tensor<Float>, Tensor<Float>) -> (Tensor<Float>, Model.AllDifferentiableVariables)\n",
    "    var lossWithGradient: LossWithGradient\n",
    "    \n",
    "    var optimizer: Opt\n",
    "    var data: Data\n",
    "    var callbacks: [Callback<Opt>]\n",
    "    \n",
    "    var loss: Tensor<Float> = Tensor(0)\n",
    "    var grad: Model.AllDifferentiableVariables = Model.AllDifferentiableVariables.zero\n",
    "    \n",
    "    var epoch: Int = 0\n",
    "    var epochs: Int = 0\n",
    "    \n",
    "    init(\n",
    "        model: Model,\n",
    "        lossWithGradient: @escaping LossWithGradient,\n",
    "        optimizer: Opt,\n",
    "        data: Data,\n",
    "        callbacks: [Callback<Opt>]\n",
    "    ) {\n",
    "        self.model = model\n",
    "        self.lossWithGradient = lossWithGradient\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "        self.callbacks = callbacks\n",
    "    }\n",
    "    \n",
    "    private func resetPerBatchValues() {\n",
    "        self.loss = Tensor(0)\n",
    "        self.grad = Model.AllDifferentiableVariables.zero        \n",
    "    }\n",
    "    \n",
    "    func trainOneBatch(xb: Tensor<Float>, yb: Tensor<Float>) -> CallbackResult {\n",
    "        var cbResult = runCallbacks(event: .beginBatch)\n",
    "        if cbResult != .proceed {\n",
    "            return cbResult\n",
    "        }\n",
    "        let context = Context(learningPhase: .training)\n",
    "        (self.loss, self.grad) = lossWithGradient(model, context, xb, yb)\n",
    "        defer {\n",
    "            // Zero out the loss & gradient to ensure stale values aren't used.\n",
    "            resetPerBatchValues()\n",
    "        }\n",
    "        cbResult = runCallbacks(event: .afterForwardsBackwards)\n",
    "        if cbResult != .proceed {\n",
    "            return cbResult\n",
    "        }\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: self.grad)\n",
    "        return .proceed\n",
    "    }\n",
    "    \n",
    "    func trainOneEpoch() -> CallbackResult {\n",
    "        switch runCallbacks(event: .beginEpoch) {\n",
    "            case .stop: return .stop\n",
    "            case .skip:\n",
    "                print(\"Unexpected .skip returned from running callbacks(event: .beginEpoch)\")\n",
    "                return .skip\n",
    "            case .proceed: break\n",
    "        }\n",
    "        for batch in self.data.trainBatches {\n",
    "            let cbResult = trainOneBatch(xb: batch.xb, yb: batch.yb)\n",
    "            if cbResult != .proceed {\n",
    "                return cbResult\n",
    "            }\n",
    "        }\n",
    "        return .proceed\n",
    "    }\n",
    "\n",
    "    func fit(epochs: Int) {\n",
    "        // I haven't implemented validation.\n",
    "        self.epochs = epochs\n",
    "        var cbResult = runCallbacks(event: .beginFit)\n",
    "        if cbResult != .proceed {\n",
    "            return\n",
    "        }\n",
    "        for epoch in 1...epochs {\n",
    "            self.epoch = epoch\n",
    "            cbResult = trainOneEpoch()\n",
    "            if cbResult != .proceed {\n",
    "                return\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    private func runCallbacks(event: CallbackEvent) -> CallbackResult {\n",
    "        for callback in callbacks {\n",
    "            let cbResult = callback.apply(event: event, learner: self)\n",
    "            if cbResult != .proceed {\n",
    "                return cbResult\n",
    "            }\n",
    "        }\n",
    "        return .proceed\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement some example callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
    "\n",
    "class Recorder<Opt: Optimizer> : Callback<Opt>\n",
    "// Hmm, this boilerplate is kind of annoying.\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input == Tensor<Float>,\n",
    "      Opt.Model.Output == Tensor<Float>,\n",
    "      // Notice that we can add constraints so that this callback only works with certain types of learners.\n",
    "      // Here, we require that the optimizer's scalar type is float so that `plt.plot` understands the\n",
    "      // learning rate.\n",
    "      Opt.Scalar == Float {\n",
    "          \n",
    "    var losses: [Float] = []\n",
    "    var lrs: [Float] = []\n",
    "          \n",
    "    override func apply(event: CallbackEvent, learner: Learner<Opt>) -> CallbackResult {\n",
    "        switch event {\n",
    "        case .beginFit:\n",
    "            losses = []\n",
    "            lrs = []\n",
    "        case .afterForwardsBackwards:\n",
    "            losses.append(learner.loss.scalar!)\n",
    "            lrs.append(learner.optimizer.learningRate)\n",
    "        default: break\n",
    "        }\n",
    "        return .proceed\n",
    "    }\n",
    "          \n",
    "    func plotLosses() {\n",
    "        plt.plot(losses)\n",
    "    }\n",
    "          \n",
    "    func plotLrs() {\n",
    "        plt.plot(lrs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler<Opt: Optimizer, Param> : Callback<Opt>\n",
    "// Hmm, this boilerplate is kind of annoying.\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input == Tensor<Float>,\n",
    "      Opt.Model.Output == Tensor<Float> {\n",
    "    \n",
    "    let paramKeyPath: ReferenceWritableKeyPath<Learner<Opt>, Param>\n",
    "    let schedule: (Float) -> Param\n",
    "    \n",
    "    init(paramKeyPath: ReferenceWritableKeyPath<Learner<Opt>, Param>, schedule: @escaping (Float) -> Param) {\n",
    "        self.paramKeyPath = paramKeyPath\n",
    "        self.schedule = schedule\n",
    "    }\n",
    "          \n",
    "    override func apply(event: CallbackEvent, learner: Learner<Opt>) -> CallbackResult {\n",
    "        switch event {\n",
    "        case .beginBatch:\n",
    "            learner[keyPath: paramKeyPath] = schedule(Float(learner.epoch) / Float(learner.epochs))\n",
    "        default: break\n",
    "        }\n",
    "        return .proceed\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sum of the two inputs is the output.\n",
    "let data = Data(trainBatches: [\n",
    "    DataBatch(xb: [[0, 1], [2, 3]], yb: [[1], [5]]),\n",
    "    DataBatch(xb: [[-3, 4], [-10, 2]], yb: [[1], [-8]]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SillyModel : Layer {\n",
    "    var dense: Dense<Float> = Dense(inputSize: 2, outputSize: 1)\n",
    "    \n",
    "    // A non-trained parameter to help illustrate the parameter scheduler.\n",
    "    @noDerivative var sillyExtraBiasParam: Float = 0\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return dense.applied(to: input, in: context) + sillyExtraBiasParam\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lossWithGrad(model: SillyModel, in context: Context, inputs: Tensor<Float>, labels: Tensor<Float>) -> (Tensor<Float>, SillyModel.AllDifferentiableVariables) {\n",
    "    return model.valueWithGradient { model -> Tensor<Float> in\n",
    "        let predictions = model.applied(to: inputs, in: context)\n",
    "        return (predictions - labels).squared().mean()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let model = SillyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Some typealiases to reduce repeatedly typing types.\n",
    "typealias MyOptimizer = SGD<SillyModel, Float>\n",
    "typealias MyLearner = Learner<MyOptimizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optimizer = MyOptimizer(learningRate: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can't schedule the learning rate because the Optimizer protocol doesn't allow setting learning rates.\n",
    "// If we change it to allow setting learning rates, `ParamScheduler` should allow setting learning rates,\n",
    "// with `paramKeyPath: \\MyLearner.optimizer.learningRate`.\n",
    "let scheduler = ParamScheduler(paramKeyPath: \\MyLearner.model.sillyExtraBiasParam) { t in\n",
    "    if t < 0.5 {\n",
    "        return -10\n",
    "    } else {\n",
    "        return 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let recorder = Recorder<MyOptimizer>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(\n",
    "    model: model,\n",
    "    lossWithGradient: lossWithGrad,\n",
    "    optimizer: optimizer,\n",
    "    data: data,\n",
    "    callbacks: [\n",
    "        recorder,\n",
    "        scheduler\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs: 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.plotLosses()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
